{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import DistilBertModel, DistilBertConfig\n",
    "from keybert import KeyBERT \n",
    "# https://maartengr.github.io/KeyBERT/guides/embeddings.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = \"\"\"\n",
    "         Social cognition concerns the various psychological processes that enable individuals to take advantage of being part of a social group. Of major importance to social cognition are the various social signals that enable us to learn about the world. Such signals include facial expressions, such as fear and disgust, which warn us of danger, and eye gaze direction, which indicate where interesting things can be found. Such signals are particularly important in infant development. Social referencing, for example, refers to the phenomenon in which infants refer to their mothers' facial expressions to determine whether or not to approach a novel object. We can learn a great deal simply by observing others. Much of this signalling seems to happen automatically and unconsciously on the part of both the sender and the receiver. We can learn to fear a stimulus by observing the response of another, in the absence of awareness of that stimulus. By contrast, learning by instruction, rather than observation, does seem to depend upon awareness of the stimulus, since such learning does not generalize to situations where the stimulus is presented subliminally. Learning by instruction depends upon a meta-cognitive process through which both the sender and the receiver recognize that signals are intended to be signals. An example would be the ‘ostensive’ signals that indicate that what follows are intentional communications. Infants learn more from signals that they recognize to be instructive. I speculate that it is this ability to recognize and learn from instructions rather than mere observation which permitted that advanced ability to benefit from cultural learning that seems to be unique to the human race.\n",
    "      \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. extract key-phrases as candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use countVectorization to split doc into candidate key words and phrases based on n-gram parameter\n",
    "\n",
    "n_gram_range = (1,3) # set keywords range from 1 to 3\n",
    "stop_words = \"english\" # remove stopping words from the doc\n",
    "\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()\n",
    "print(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternative\n",
    "# # the keyphrase-vectorizers pkg auto decide n-gram without need to specify\n",
    "\n",
    "# Init default vectorizer.\n",
    "vectorizer = KeyphraseCountVectorizer()\n",
    "\n",
    "# Print parameters\n",
    "# print(vectorizer.get_params())\n",
    "\n",
    "# fit to learn keywords\n",
    "fitted_vectorizer = vectorizer.fit([doc])\n",
    "\n",
    "# # After learning the keyphrases, they can be returned.\n",
    "candicates = fitted_vectorizer.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. select embedding models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# here we use distilbert model from sentencetransformer. It is lightweight.\n",
    "# initiate model\n",
    "sentence_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "# embedding [what does embedding do? ]\n",
    "doc_embedding = sentence_model.encode([doc])\n",
    "candidate_embeddings = sentence_model.encode(candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute cosine similarity - how similar are the candidate words? \n",
    "n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-n:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Max Sum Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To diversify the results, we take the 2 x top_n most similar words/phrases to the document. Then, we take all top_n combinations from the 2 x top_n words and extract the combination that are the least similar to each other by cosine similarity.\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. choose code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('various social signals', 0.4425),\n",
       " ('social cognition', 0.4335),\n",
       " ('cultural learning', 0.4112),\n",
       " ('various psychological processes', 0.4108),\n",
       " ('social referencing', 0.3608)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = KeyphraseCountVectorizer()\n",
    "sentence_model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "kw_model = KeyBERT(model=sentence_model)\n",
    "kw_model.extract_keywords(doc, vectorizer=vectorizer,  top_n=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tfenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "704e9b2975f7d9c284f1f72782e181ed31e6cab7271a1f7f351a1ecf00ecbeb1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
